\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort&compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{paralist}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{sidecap}
\sidecaptionvpos{figure}{t}

\usepackage{wrapfig}
\usepackage{todonotes}

\usepackage{caption}
\usepackage{subcaption}
% \usepackage[backend=bibtex]{biblatex}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{import}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}


\input{math_commands.tex}

\usepackage{amsthm}
\usepackage{amsfonts, amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bibentry}
\usepackage{float}
\usepackage{dsfont}
\usepackage{setspace}
\usepackage{todonotes}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{cleveref}
\nobibliography*
%\usepackage[group-separator={,}]{siunitx}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Meta-PDE: Learning to Solve PDEs Quickly Without a Mesh}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
%\textcolor{red}{need to rewrite...}
Partial differential equations (PDEs) are powerful mathematical tools for expressing our physical understanding of many systems, both natural and engineered.
Solving PDEs, however, is often computationally challenging and typically requires tools such as finite element analysis (FEA).
Recently, neural networks have shown promise as alternative classes of function approximators for finding PDE solutions, with potential advantages including native GPU support, a highly flexible mesh-free basis, and efficient tooling based on automatic differentiation.
In this work, we observe that in many settings---notably design and optimization problems---solving a single PDE is actually not the goal, but instead many related PDEs must be be solved for a variety of, e.g., boundary conditions or geometric domains.
To take advantage of the cross-task structure present in such problems, we present a meta-learning based method for rapidly fitting surrogate models to collections of related boundary value problems.
Our approach is based on an alternative API for surrogate modeling which takes as input 1) a sampler for points in the domain, and 2) a variational energy density which measures deviation from the governing equations.
We use meta-learning (MAML and LEAP) to identify neural network initializations such that the residual of the PDE can be minimized on a novel task with just a few gradient steps, without requiring supervision from expensive ground-truth or FEA solutions.
We apply our ``meta-solving'' approach to nonlinear Poisson's Equation, Burger's Equations, and Hyperelasticity Equations, and show that it is often capable of solving PDEs accurately and quickly across different boundary conditions, governing equations, and problem geometries.
The resulting Meta-PDE Method solves these PDEs orders of magnitude faster than FEA methods which achieve similar accuracy.
\end{abstract}
\input{sections/1_intro}
%\input{sections/2_fea}
%\input{sections/3_surrogates}
%\input{sections/4_pinn}
\input{sections/5_metapde}
\input{sections/6_experiments}
\input{sections/7_discussion}
\input{sections/8_conclusion}

\bibliographystyle{plainnat}
\bibliography{references}
\newpage
\appendix
\input{sections/9_appendix}

\end{document}
