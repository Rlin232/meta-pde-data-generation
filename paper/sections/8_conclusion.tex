\section{Conclusion}
We presented Meta-PDE, a surrogate model uses meta-learning to amortize PDE solving across classes of PDEs with complex and varying geometries and governing equations. Meta-PDE takes as input the governing equations themselves and a sampler for the PDE domain and boundary, thus remaining as close as possible in API to the fundamental representation of the PDE in terms of governing equations and domain. This avoids having to fix a parametric representation of geometry, governing equations and solution for the class of PDEs to be amortized. We show on three problems: nonlinear Poisson's Equation, Burger's Equation and Hyperelasiticty Equation that Meta-PDE can learn to output accurate solutions. In some cases, we observe a significantly more favorable accuracy/speed trade-off than a baseline finite element solver. The Meta-PDE method consists of two different implementations, differing in their meta-learning algorithms (MAML v.s. LEAP). Based on the three sets of experimentation, we observe that MAML-based Meta-PDE method yields superior speed/accuracy trade-off during deployment time. However, it comes as a cost of higher memory footprint during meta-training. The high-memory footprint imposes challanges when one want to use larger or more complex neural network architectures to solve complex PDE problems. The meta-training time for MAML-based Meta-PDE method is also slightly longer than its LEAP counterpart. The LEAP-based Meta-PDE method, on the other hand, has a worse speed/accuracy trade-off during deployment time but the meta-training time is slightly shorter. However, since the inner-loop learning rate is not meta-learnable for LEAP, the LEAP-based Meta-PDE training process is more sensitive to this hyperparameters. 